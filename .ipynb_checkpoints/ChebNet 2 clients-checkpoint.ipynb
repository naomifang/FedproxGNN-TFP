{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Anaconda\\Lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "D:\\Anaconda\\Lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_processing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7bb47f5a5b1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_processing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLoadData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msyft\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data_processing'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from data_processing import LoadData\n",
    "from model import *\n",
    "import syft as sy\n",
    "import copy\n",
    "hook = sy.TorchHook(torch)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    \n",
    "    dataset = pd.read_csv('PeMS/PeMS04.csv')\n",
    "    dataset_size = len(dataset)\n",
    "\n",
    "    indices = list(range(dataset_size))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split1 = int(np.floor(0.6 * dataset_size)) #60-40 Bob-Alice\n",
    "    bobs_indices, alices_indices = indices[:split1], indices[split1:]\n",
    "\n",
    "    bobs_sampler = SubsetRandomSampler(bobs_indices)\n",
    "    alices_sampler = SubsetRandomSampler(alices_indices)\n",
    "\n",
    "    train_data = LoadData(data_path=[\"PeMS_04/PeMS04.csv\", \"PeMS_04/PeMS04.npz\"], num_nodes=307, divide_days=[45, 14],\n",
    "                              time_interval=5, history_length=6,\n",
    "                              train_mode=\"train\")\n",
    "\n",
    "\n",
    "    bobs_loader = DataLoader(train_data, batch_size=64, num_workers=32, sampler=bobs_sampler)\n",
    "    alices_loader = DataLoader(train_data, batch_size=64, num_workers=32, sampler=alices_sampler)\n",
    "\n",
    "#     # Loading Dataset\n",
    "#     train_data = LoadData(data_path=[\"PeMS_04/PeMS04.csv\", \"PeMS_04/PeMS04.npz\"], num_nodes=307, divide_days=[45, 14],\n",
    "#                           time_interval=5, history_length=6,\n",
    "#                           train_mode=\"train\")\n",
    "\n",
    "#     train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=32)\n",
    "#....................................................\n",
    "#     test_data = LoadData(data_path=[\"PeMS_04/PeMS04.csv\", \"PeMS_04/PeMS04.npz\"], num_nodes=307, divide_days=[45, 14],\n",
    "#                          time_interval=5, history_length=6,\n",
    "#                          train_mode=\"test\")\n",
    "\n",
    "#     test_loader = DataLoader(test_data, batch_size=64, shuffle=True, num_workers=32)\n",
    "    \n",
    "#     for data in train_loader: \n",
    "#         print(data)\n",
    "\n",
    "    #model = GCN(in_c=6 , hid_c=6 ,out_c=1)\n",
    "    #ChebNet(in_c=6, hid_c=32, out_c=1, K=2)\n",
    "    #GATNet(in_c=6 , hid_c=6 ,out_c=1, n_heads=1)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    bobs_model = ChebNet(in_c=6, hid_c=32, out_c=1, K=2)     # 2阶切比雪夫模型    \n",
    "    bobs_model = bobs_model.to(device)\n",
    "    \n",
    "    alices_model = ChebNet(in_c=6, hid_c=32, out_c=1, K=2)      # 2阶切比雪夫模型\n",
    "    alices_model = alices_model.to(device)\n",
    "\n",
    "    #new\n",
    "    bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "    alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "    secure_worker = sy.VirtualWorker(hook, id=\"secure_worker\")\n",
    "\n",
    "    #new\n",
    "#     bobs_model = model.copy().send(bob)\n",
    "#     alices_model = model.copy().send(alice)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    #new\n",
    "    bobs_opt = optim.Adam(params=bobs_model.parameters())\n",
    "    alices_opt = optim.Adam(params=alices_model.parameters())\n",
    "\n",
    "    # Train model\n",
    "    Epoch = 8\n",
    "\n",
    "    bobs_model.train()\n",
    "    alices_model.train()\n",
    "    for epoch in range(Epoch):\n",
    "    \t#new\n",
    "        epoch_loss_bob = 0.0\n",
    "        epoch_loss_alice = 0.0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for data in bobs_loader:  # [\"graph\": [B, N, N] , \"flow_x\": [B, N, H, D], \"flow_y\": [B, N, 1, D]]\n",
    "            bobs_model.zero_grad()\n",
    "            predict_value = bobs_model(data, device).to(torch.device(\"cpu\"))  # [0, 1] -> recover\n",
    "            loss = criterion(predict_value, data[\"flow_y\"])\n",
    "            epoch_loss_bob += loss.item()\n",
    "            loss.backward()\n",
    "            bobs_opt.step()\n",
    "            \n",
    "        for data in alices_loader:\n",
    "            alices_model.zero_grad()\n",
    "            predict_value = alices_model(data, device).to(torch.device(\"cpu\"))  # [0, 1] -> recover\n",
    "            loss = criterion(predict_value, data[\"flow_y\"])\n",
    "            epoch_loss_alice += loss.item()\n",
    "            loss.backward()\n",
    "            alices_opt.step()\n",
    "            \n",
    "        end_time = time.time()\n",
    "\n",
    "        print(\"Epoch: {:04d}, Loss Bob: {:02.4f}, Loss Alice: {:02.4f}, Time: {:02.2f} mins\".format(epoch, 1000 * epoch_loss_bob / len(train_data), 1000 * epoch_loss_alice / len(train_data),\n",
    "                                                                          (end_time-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000, Loss Bob: 0.0241, Loss Alice: 0.0157, Time: 2.82 mins\n",
      "Epoch: 0001, Loss Bob: 0.0177, Loss Alice: 0.0136, Time: 2.55 mins\n",
      "Epoch: 0002, Loss Bob: 0.0178, Loss Alice: 0.0134, Time: 2.55 mins\n",
      "Epoch: 0003, Loss Bob: 0.0149, Loss Alice: 0.0102, Time: 2.54 mins\n",
      "Epoch: 0004, Loss Bob: 0.0123, Loss Alice: 0.0099, Time: 2.54 mins\n",
      "Epoch: 0005, Loss Bob: 0.0093, Loss Alice: 0.0122, Time: 2.62 mins\n",
      "Epoch: 0006, Loss Bob: 0.0063, Loss Alice: 0.0089, Time: 2.56 mins\n",
      "Epoch: 0007, Loss Bob: 0.0047, Loss Alice: 0.0074, Time: 2.56 mins\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
