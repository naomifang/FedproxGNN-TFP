{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ca5a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from data_processing import LoadData\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "seed = 1001\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00783f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN模型\n",
    "class GCN(nn.Module): # GCN模型，向空域的第一个图卷积\n",
    "    def __init__(self, in_c, hid_c, out_c):\n",
    "        super(GCN, self).__init__() # 表示继承父类的所有属性和方法\n",
    "        self.linear_1 = nn.Linear(in_c, hid_c) # 定义一个线性层\n",
    "        self.linear_2 = nn.Linear(hid_c, out_c) # 定义一个线性层\n",
    "        self.act = nn.ReLU() # 定义激活函数\n",
    "\n",
    "    def forward(self, data, device):\n",
    "        graph_data = data[\"graph\"].to(device)[0]  # [N, N] 邻接矩阵，并且将数据送入设备\n",
    "        graph_data = GCN.process_graph(graph_data)  # 变换邻接矩阵 \\hat A = D_{-1/2}*A*D_{-1/2}\n",
    "\n",
    "        flow_x = data[\"flow_x\"].to(device)  # [B, N, H, D]  流量数据\n",
    "\n",
    "        B, N = flow_x.size(0), flow_x.size(1) # batch_size、节点数\n",
    "\n",
    "        flow_x = flow_x.view(B, N, -1)  # [B, N, H*D] H = 6, D = 1把最后两维缩减到一起了，这个就是把历史时间的特征放一起\n",
    "\n",
    "       # 第一个图卷积层\n",
    "        output_1 = self.linear_1(flow_x)  # [B, N, hid_C],这个就是 WX，其中W是可学习的参数，X是输入的流量数据（就是flow_x）\n",
    "        output_1 = self.act(torch.matmul(graph_data, output_1))  # [B, N, N] ,[B, N, hid_c]，就是 \\hat AWX\n",
    "       \n",
    "        # 第二个图卷积层\n",
    "        output_2 = self.linear_2(output_1) # WX\n",
    "        output_2 = self.act(torch.matmul(graph_data, output_2))   # [B, N, 1, Out_C] , 就是 \\hat AWX\n",
    "\n",
    "        return output_2.unsqueeze(2)  # 第２维的维度扩张\n",
    "\n",
    "    @staticmethod\n",
    "    def process_graph(graph_data): # 这个就是在原始的邻接矩阵之上，再次变换，也就是\\hat A = D_{-1/2}*A*D_{-1/2}\n",
    "        N = graph_data.size(0) # 获得节点的个数\n",
    "        matrix_i = torch.eye(N, dtype=graph_data.dtype, device=graph_data.device)# 定义[N, N]的单位矩阵\n",
    "        graph_data += matrix_i  # [N, N]  ,就是 A+I \n",
    "\n",
    "        degree_matrix = torch.sum(graph_data, dim=-1, keepdim=False)  # [N]#[N],计算度矩阵，塌陷成向量，其实就是将上面的A+I每行相加\n",
    "        degree_matrix = degree_matrix.pow(-1) # 计算度矩阵的逆，若为0，-1次方可能计算结果为无穷大的数\n",
    "        degree_matrix[degree_matrix == float(\"inf\")] = 0.  # 让无穷大的数为0\n",
    "\n",
    "        degree_matrix = torch.diag(degree_matrix)  # [N, N]# 转换成对角矩阵\n",
    "\n",
    "        return torch.mm(degree_matrix, graph_data)  # D^(-1) * A = \\hat(A) # 返回 \\hat A=D^(-1) * A ,这个等价于\\hat A = D_{-1/2}*A*D_{-1/2}\n",
    "\n",
    "\n",
    "class Baseline(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(Baseline, self).__init__()\n",
    "        self.layer = nn.Linear(in_c, out_c)\n",
    "\n",
    "    def forward(self, data, device):\n",
    "        flow_x = data[\"flow_x\"].to(device)  # [B, N, H, D]\n",
    "\n",
    "        B, N = flow_x.size(0), flow_x.size(1)\n",
    "\n",
    "        flow_x = flow_x.view(B, N, -1)  # [B, N, H*D]  H = 6, D = 1\n",
    "\n",
    "        output = self.layer(flow_x)  # [B, N, Out_C], Out_C = D\n",
    "\n",
    "        return output.unsqueeze(2)  # [B, N, 1, D=Out_C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d05b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT模型\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_c = in_c\n",
    "        self.out_c = out_c\n",
    "\n",
    "        self.F = F.softmax\n",
    "\n",
    "        self.W = nn.Linear(in_c, out_c, bias=False)  # y = W * x\n",
    "        self.b = nn.Parameter(torch.Tensor(out_c))\n",
    "\n",
    "        nn.init.normal_(self.W.weight)\n",
    "        nn.init.normal_(self.b)\n",
    "\n",
    "    def forward(self, inputs, graph):\n",
    "        \"\"\"\n",
    "        :param inputs: input features, [B, N, C].\n",
    "        :param graph: graph structure, [N, N].\n",
    "        :return:\n",
    "            output features, [B, N, D].\n",
    "        \"\"\"\n",
    "\n",
    "        h = self.W(inputs)  # [B, N, D]，一个线性层，就是第一步中公式的 W*h\n",
    "        \n",
    "        # 下面这个就是，第i个节点和第j个节点之间的特征做了一个内积，表示它们特征之间的关联强度\n",
    "        # 再用graph也就是邻接矩阵相乘，因为邻接矩阵用0-1表示，0就表示两个节点之间没有边相连\n",
    "        # 那么最终结果中的0就表示节点之间没有边相连\n",
    "        outputs = torch.bmm(h, h.transpose(1, 2)) * graph.unsqueeze(0)  #  [B, N, D]*[B, D, N]->[B, N, N],         x(i)^T * x(j)\n",
    "        \n",
    "        # 由于上面计算的结果中0表示节点之间没关系，所以将这些0换成负无穷大，因为softmax的负无穷大=0\n",
    "        outputs.data.masked_fill_(torch.eq(outputs, 0), -float(1e16))   \n",
    "\n",
    "        attention = self.F(outputs, dim=2)   # [B, N, N]，在第２维做归一化，就是说所有有边相连的节点做一个归一化，得到了注意力系数\n",
    "        return torch.bmm(attention, h) + self.b  # [B, N, N] * [B, N, D]，，这个是第三步的，利用注意力系数对邻域节点进行有区别的信息聚合\n",
    "\n",
    "\n",
    "class GATSubNet(nn.Module): #这个是多头注意力机制\n",
    "    def __init__(self, in_c, hid_c, out_c, n_heads):\n",
    "        super(GATSubNet, self).__init__()\n",
    "\n",
    "# 　      用循环来增加多注意力， 用nn.ModuleList变成一个大的并行的网络\n",
    "        self.attention_module = nn.ModuleList([GraphAttentionLayer(in_c, hid_c) for _ in range(n_heads)]) # in_c为输入特征维度，hid_c为隐藏层特征维度\n",
    "\n",
    "        # 上面的多头注意力都得到了不一样的结果，使用注意力层给聚合起来\n",
    "        self.out_att = GraphAttentionLayer(hid_c * n_heads, out_c)\n",
    "\n",
    "        self.act = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, inputs, graph):\n",
    "        \"\"\"\n",
    "        :param inputs: [B, N, C]\n",
    "        :param graph: [N, N]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 每一个注意力头用循环取出来，放入list里，然后在最后一维串联起来\n",
    "        outputs = torch.cat([attn(inputs, graph) for attn in self.attention_module], dim=-1)  # [B, N, hid_c * h_head]\n",
    "        outputs = self.act(outputs)\n",
    "\n",
    "        outputs = self.out_att(outputs, graph)\n",
    "\n",
    "        return self.act(outputs)\n",
    "\n",
    "\n",
    "class GATNet(nn.Module):\n",
    "    def __init__(self, in_c, hid_c, out_c, n_heads):\n",
    "        super(GATNet, self).__init__()\n",
    "        self.subnet = GATSubNet(in_c, hid_c, out_c, n_heads)\n",
    "        # self.subnet = [GATSubNet(...) for _ in range(T)]\n",
    "\n",
    "    def forward(self, data, device):\n",
    "        graph = data[\"graph\"][0].to(device)  # [N, N]\n",
    "        flow = data[\"flow_x\"]  # [B, N, T, C]\n",
    "        flow = flow.to(device)  # 将流量数据送入设备\n",
    "\n",
    "        B, N = flow.size(0), flow.size(1)\n",
    "        flow = flow.view(B, N, -1)  # [B, N, T * C]\n",
    "        \"\"\"\n",
    "        上面是将这一段的时间的特征数据摊平做为特征，这种做法实际上忽略了时序上的连续性\n",
    "        这种做法可行，但是比较粗糙，当然也可以这么做：\n",
    "        flow[:, :, 0] ... flow[:, :, T-1]   则就有T个[B, N, C]这样的张量，也就是 [B, N, C]*T\n",
    "        每一个张量都用一个SubNet来表示，则一共有T个SubNet，初始化定义　self.subnet = [GATSubNet(...) for _ in range(T)]\n",
    "        然后用nn.ModuleList将SubNet分别拎出来处理，参考多头注意力的处理，同理\n",
    "        \n",
    "        \"\"\"\n",
    "        prediction = self.subnet(flow, graph).unsqueeze(2)  # [B, N, 1, C]，这个１加上就表示预测的是未来一个时刻\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7577e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chebnet模型\n",
    "import torch.nn.init as init\n",
    "class ChebConv(nn.Module): # 定义图卷积层的类\n",
    "    \"\"\"\n",
    "    The ChebNet convolution operation.\n",
    "\n",
    "    :param in_c: int, number of input channels.\n",
    "    :param out_c: int, number of output channels.\n",
    "    :param K: int, Chebyshev多项式的阶数。\n",
    "    \"\"\"\n",
    "    def __init__(self, in_c, out_c, K, bias=True, normalize=True):\n",
    "        super(ChebConv, self).__init__()\n",
    "        self.normalize = normalize # 正则化参数,True or False\n",
    "\n",
    "        self.weight = nn.Parameter(torch.Tensor(K + 1, 1, in_c, out_c))  # [K+1, 1, in_c, out_c] ,第二个1是维度扩张，计算方便,有没有都不影响参数的大小,nn.Parameter就是把参数转换成模型可改动的参数.\n",
    "        # 之所以要k+1,是因为k是从0开始的\n",
    "        init.xavier_normal_(self.weight)  # 用正态分布填充\n",
    "\n",
    "        if bias: # 偏置,就是一次函数中的b\n",
    "            self.bias = nn.Parameter(torch.Tensor(1, 1, out_c))  # 前面的两个1是为了计算简单，因为输出的维度是3维\n",
    "            init.zeros_(self.bias)  # 用0填充\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        self.K = K + 1\n",
    "\n",
    "    def forward(self, inputs, graph):\n",
    "        \"\"\"\n",
    "        :param inputs: the input data, [B, N, C]\n",
    "        :param graph: the graph structure, [N, N]\n",
    "        :return: convolution result, [B, N, D]\n",
    "        \"\"\"\n",
    "        L = ChebConv.get_laplacian(graph, self.normalize)  # [N, N],得到拉普拉斯矩阵\n",
    "        mul_L = self.cheb_polynomial(L).unsqueeze(1)   # [K, 1, N, N]，这个就是多阶的切比雪夫多项式，K就是阶数，N是节点数量\n",
    "\n",
    "        result = torch.matmul(mul_L, inputs)  # [K, B, N, C]，这个就是计算完后乘x\n",
    "        result = torch.matmul(result, self.weight)  # [K, B, N, D]，计算上一步之后乘W\n",
    "        result = torch.sum(result, dim=0) + self.bias  # [B, N, D]，求和\n",
    "\n",
    "        return result\n",
    "\n",
    "    def cheb_polynomial(self, laplacian): # 计算切比雪夫多项式,也就是前面公式中的 T_k(L)\n",
    "        \"\"\"\n",
    "        Compute the Chebyshev Polynomial, according to the graph laplacian.\n",
    "\n",
    "        :param laplacian: the graph laplacian, [N, N].\n",
    "        :return: the multi order Chebyshev laplacian, [K, N, N].\n",
    "        \"\"\"\n",
    "        N = laplacian.size(0)  # [N, N] ,节点个数\n",
    "        multi_order_laplacian = torch.zeros([self.K, N, N], device=laplacian.device, dtype=torch.float)  # [K, N, N],初始化一个全0的多项式拉普拉斯矩阵\n",
    "        multi_order_laplacian[0] = torch.eye(N, device=laplacian.device, dtype=torch.float)  # 0阶的切比雪夫多项式为单位阵\n",
    "\n",
    "        if self.K == 1: # 这个self.k就是前面说的0阶切比雪夫多项式\n",
    "            return multi_order_laplacian\n",
    "        else: # 大于等于1阶\n",
    "            multi_order_laplacian[1] = laplacian\n",
    "            if self.K == 2: # 1阶切比雪夫多项式就是拉普拉斯矩阵 L 本身\n",
    "                return multi_order_laplacian\n",
    "            else:\n",
    "                for k in range(2, self.K):\n",
    "                    multi_order_laplacian[k] = 2 * torch.mm(laplacian, multi_order_laplacian[k-1]) - \\\n",
    "                                               multi_order_laplacian[k-2] #切比雪夫多项式的递推式:T_k(L) = 2 * L * T_{k-1}(L) - T_{k-2}(L)\n",
    "\n",
    "        return multi_order_laplacian\n",
    "\n",
    "    @staticmethod\n",
    "    def get_laplacian(graph, normalize): # 计算拉普拉斯矩阵\n",
    "        \"\"\"\n",
    "        return the laplacian of the graph.\n",
    "\n",
    "        :param graph: the graph structure without self loop, [N, N].\n",
    "        :param normalize: whether to used the normalized laplacian.\n",
    "        :return: graph laplacian.\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            D = torch.diag(torch.sum(graph, dim=-1) ** (-1 / 2)) # 这里的graph就是邻接矩阵,这个D\n",
    "            L = torch.eye(graph.size(0), device=graph.device, dtype=graph.dtype) - torch.mm(torch.mm(D, graph), D) # L = I - D * A * D,这个也就是正则化\n",
    "        else:\n",
    "            D = torch.diag(torch.sum(graph, dim=-1))\n",
    "            L = D - graph\n",
    "        return L\n",
    "\n",
    "\n",
    "class ChebNet(nn.Module):  # 定义图网络的类\n",
    "    def __init__(self, in_c, hid_c, out_c, K):\n",
    "        \"\"\"\n",
    "        :param in_c: int, number of input channels.\n",
    "        :param hid_c: int, number of hidden channels.class\n",
    "        :param out_c: int, number of output channels.\n",
    "        :param K:\n",
    "        \"\"\"\n",
    "        super(ChebNet, self).__init__()\n",
    "        self.conv1 = ChebConv(in_c=in_c, out_c=hid_c, K=K) # 第一个图卷积层\n",
    "        self.conv2 = ChebConv(in_c=hid_c, out_c=out_c, K=K)  # 第二个图卷积层\n",
    "        self.act = nn.ReLU() # 激活函数\n",
    "\n",
    "    def forward(self, data, device):\n",
    "        graph_data = data[\"graph\"].to(device)[0]  # [N, N]\n",
    "        flow_x = data[\"flow_x\"].to(device)  # [B, N, H, D]  # B是batch size，N是节点数，H是历史数据长度，D是特征维度\n",
    "\n",
    "        B, N = flow_x.size(0), flow_x.size(1) \n",
    "\n",
    "        flow_x = flow_x.view(B, N, -1)  # [B, N, H*D] H = 6, D = 1把最后两维缩减到一起了，这个就是把历史时间的特征放一起\n",
    "\n",
    "        output_1 = self.act(self.conv1(flow_x, graph_data))\n",
    "        output_2 = self.act(self.conv2(output_1, graph_data))\n",
    "\n",
    "        return output_2.unsqueeze(2)  # 在第２维度，也就是时间维度上做扩张"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e42ecc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "def eva_regress(y_true, y_pred, accuracy, loss):\n",
    "    recall = metrics.recall_score(y_true, y_pred, average='macro')\n",
    "    precision = metrics.precision_score(y_true, y_pred, average='macro')\n",
    "    f1 = metrics.f1_score(y_true, y_pred, average='weighted')\n",
    "    kappa = metrics.cohen_kappa_score(y_true, y_pred)\n",
    "    jaccard = metrics.jaccard_similarity_score(y_true, y_pred)\n",
    "    print(recall,precision,f1,accuracy,loss,kappa,jaccard, end='\\\\t')\n",
    "\n",
    "def pre(y_pred):\n",
    "    y_pred_ = y_pred\n",
    "    for i in range(len(y_pred_)):\n",
    "        max_value = max(y_pred_[i])\n",
    "        for j in range(len(y_pred_[i])):\n",
    "            if max_value == y_pred_[i][j]:\n",
    "                y_pred_[i][j] = 1\n",
    "            else:\n",
    "                y_pred_[i][j] = 0\n",
    "    return y_pred_\n",
    "\n",
    "def one_hot_to_array(one_hots):\n",
    "    data = [np.argmax(one_hot) for one_hot in one_hots]\n",
    "    return data\n",
    "\n",
    "def MAE(y_true,y_pre):\n",
    "    y_true=(y_true).detach().numpy().copy().reshape((-1,1))\n",
    "    y_pre=(y_pre).detach().numpy().copy().reshape((-1,1))\n",
    "    re = np.abs(y_true-y_pre).mean()\n",
    "    return re\n",
    "\n",
    "def RMSE(y_true,y_pre):\n",
    "    y_true=(y_true).detach().numpy().copy().reshape((-1,1))\n",
    "    y_pre=(y_pre).detach().numpy().copy().reshape((-1,1))\n",
    "    re = math.sqrt(((y_true-y_pre)**2).mean())\n",
    "    return re\n",
    "\n",
    "def MAPE(y_true,y_pre):\n",
    "    y_true=(y_true).detach().numpy().copy().reshape((-1,1))\n",
    "    y_pre=(y_pre).detach().numpy().copy().reshape((-1,1))\n",
    "    e = (y_true+y_pre)/2+1e-2\n",
    "    re = (np.abs(y_true-y_pre)/(np.abs(y_true)+e)).mean()\n",
    "    return re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "483362ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "    # Loading Dataset\n",
    "    train_data = LoadData(data_path=[\"PeMS/PeMS04.csv\", \"PeMS/PeMS04.npz\"], num_nodes=307, divide_days=[45, 14],\n",
    "                          time_interval=5, history_length=6,\n",
    "                          train_mode=\"train\")\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "    test_data = LoadData(data_path=[\"PeMS/PeMS04.csv\", \"PeMS/PeMS04.npz\"], num_nodes=307, divide_days=[45, 14],\n",
    "                         time_interval=5, history_length=6,\n",
    "                         train_mode=\"test\")\n",
    "\n",
    "    test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Loading Model\n",
    "    # TODO:  Construct the GAT (must) and DCRNN (optional) Model\n",
    "\n",
    "    #my_net = None\n",
    "    my_net = GATNet(6,6,1,2)\n",
    "    #my_net = GCN(6,6,1)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    my_net = my_net.to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = optim.Adam(params=my_net.parameters())\n",
    "\n",
    "    # Train model\n",
    "    Adam_Epoch = 5\n",
    "    my_net.train()\n",
    "    for epoch in range(Adam_Epoch):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_mae = 0.0\n",
    "        epoch_rmse = 0.0\n",
    "        epoch_mape = 0.0\n",
    "        num = 0\n",
    "        start_time = time.time()\n",
    "        for data in train_loader:  # [\"graph\": [B, N, N] , \"flow_x\": [B, N, H, D], \"flow_y\": [B, N, 1, D]]\n",
    "            my_net.zero_grad()\n",
    "            predict_value = my_net(data, device).to(torch.device(\"cpu\"))  # [0, 1] -> recover\n",
    "\n",
    "            loss = criterion(predict_value, data[\"flow_y\"])      \n",
    "            epoch_mae += MAE( data[\"flow_y\"],predict_value)\n",
    "            epoch_rmse += RMSE( data[\"flow_y\"],predict_value)\n",
    "            epoch_mape += MAPE( data[\"flow_y\"],predict_value)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        end_time = time.time()\n",
    "        epoch_mae = epoch_mae/num\n",
    "        epoch_rmse = epoch_rmse/num\n",
    "        epoch_mape = epoch_mape/num\n",
    "        print(\"Epoch: {:04d}, Loss: {:02.4f}, mae: {:02.4f}, rmse: {:02.4f}, mape: {:02.4f}, Time: {:02.2f} mins\".format(epoch+1,  10*epoch_loss / (len(train_data)/64),\n",
    "                                                                                                              epoch_mae,epoch_rmse,epoch_mape,(end_time-start_time)/60))\n",
    "    SGD_epoch = 15\n",
    "    optimizer = torch.optim.SGD(my_net.parameters(), lr=0.2,weight_decay=0.005)\n",
    "    for epoch in range(SGD_epoch):\n",
    "        if (epoch % 3 == 0):\n",
    "            for p in optimizer.param_groups:\n",
    "                p['lr'] *= 0.5\n",
    "        epoch_loss = 0.0\n",
    "        epoch_mae = 0.0\n",
    "        epoch_rmse = 0.0\n",
    "        epoch_mape = 0.0\n",
    "        num = 0\n",
    "        start_time = time.time()\n",
    "        for data in train_loader:  # [\"graph\": [B, N, N] , \"flow_x\": [B, N, H, D], \"flow_y\": [B, N, 1, D]]\n",
    "            my_net.zero_grad()\n",
    "            predict_value = my_net(data, device).to(torch.device(\"cpu\"))  # [0, 1] -> recover\n",
    "\n",
    "            loss = criterion(predict_value, data[\"flow_y\"])      \n",
    "            epoch_mae += MAE( data[\"flow_y\"],predict_value)\n",
    "            epoch_rmse += RMSE( data[\"flow_y\"],predict_value)\n",
    "            epoch_mape += MAPE( data[\"flow_y\"],predict_value)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        end_time = time.time()\n",
    "        epoch_mae = epoch_mae/num\n",
    "        epoch_rmse = epoch_rmse/num\n",
    "        epoch_mape = epoch_mape/num\n",
    "        print(\"Epoch: {:04d}, Loss: {:02.4f}, mae: {:02.4f}, rmse: {:02.4f}, mape: {:02.4f}, Time: {:02.2f} mins\".format(epoch+Adam_Epoch+1, 10*epoch_loss / (len(train_data)/64),\n",
    "                                                                                                              epoch_mae,epoch_rmse,epoch_mape,(end_time-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3562e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    my_net.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_mae = 0.0\n",
    "        epoch_rmse = 0.0\n",
    "        epoch_mape = 0.0\n",
    "        total_loss = 0.0\n",
    "        num = 0\n",
    "        all_predict_value = 0\n",
    "        all_y_true = 0\n",
    "        for data in test_loader:\n",
    "            predict_value = my_net(data, device).to(torch.device(\"cpu\"))\n",
    "            if num == 0:\n",
    "                all_predict_value = predict_value\n",
    "                all_y_true = data[\"flow_y\"]\n",
    "            else:\n",
    "                all_predict_value = torch.cat([all_predict_value, predict_value], dim=0)\n",
    "                all_y_true = torch.cat([all_y_true, data[\"flow_y\"]], dim=0)\n",
    "            loss = criterion(predict_value, data[\"flow_y\"])\n",
    "            total_loss += loss.item()\n",
    "            num += 1\n",
    "        epoch_mae = MAE( all_y_true,all_predict_value)\n",
    "        epoch_rmse = RMSE( all_y_true,all_predict_value)\n",
    "        epoch_mape = MAPE( all_y_true,all_predict_value)\n",
    "        print(\"Test Loss: {:02.4f}, mae: {:02.4f}, rmse: {:02.4f}, mape: {:02.4f}\".format( 10*total_loss / (len(test_data)/64) ,epoch_mae,epoch_rmse,epoch_mape))\n",
    "\n",
    "    #save the model\n",
    "    torch.save(my_net,'model_GAT_6.pth')\n",
    "\n",
    "\n",
    "    ####choose node to visualize\n",
    "    node_id = 120\n",
    "\n",
    "    plt.title(\"Node\" + str(node_id)+\" visualization for 1 day\")\n",
    "    plt.xlabel(\"time/5min\")\n",
    "    plt.ylabel(\"traffic flow\")\n",
    "    plt.plot(test_data.recover_data(test_data.flow_norm[0],test_data.flow_norm[1],all_y_true)[:24*12,node_id,0,0],label='Ground Truth')\n",
    "    plt.plot(test_data.recover_data(test_data.flow_norm[0],test_data.flow_norm[1],all_predict_value)[:24*12,node_id,0,0],label = 'Prediction')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"Node\" + str(node_id)+\" visualization for 1 day.png\", dpi=400)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title(\"Node\" + str(node_id)+\" visualization (2 weeks)\")\n",
    "    plt.xlabel(\"time/5min\")\n",
    "    plt.ylabel(\"traffic flow\")\n",
    "    plt.plot(test_data.recover_data(test_data.flow_norm[0],test_data.flow_norm[1],all_y_true)[:,node_id,0,0],label = 'Ground Truth')\n",
    "    plt.plot(test_data.recover_data(test_data.flow_norm[0],test_data.flow_norm[1],all_predict_value)[:,node_id,0,0],label = 'Prediction')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"Node\" + str(node_id)+\" visualization for 2 weeks.png\", dpi=400)\n",
    "    plt.show()\n",
    "\n",
    "    mae = MAE(test_data.recover_data(test_data.flow_norm[0],test_data.flow_norm[1],all_y_true),test_data.recover_data(test_data.flow_norm[0],test_data.flow_norm[1],all_predict_value))\n",
    "    rmse = RMSE(test_data.recover_data(test_data.flow_norm[0],test_data.flow_norm[1],all_y_true),test_data.recover_data(test_data.flow_norm[0],test_data.flow_norm[1],all_predict_value))\n",
    "    mape = MAPE(test_data.recover_data(test_data.flow_norm[0],test_data.flow_norm[1],all_y_true),test_data.recover_data(test_data.flow_norm[0],test_data.flow_norm[1],all_predict_value))\n",
    "    print(\"Accuracy Indicators Based on Original Values  mae: {:02.4f}, rmse: {:02.4f}, mape: {:02.4f}\".format(mae,rmse,mape))\n",
    "    return mae,rmse,mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "579e3fa7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'n_heads'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-56ae4f7afcc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-89d79cb04044>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m#my_net = None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mmy_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGATNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;31m#my_net = GCN(6,6,1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'n_heads'"
     ]
    }
   ],
   "source": [
    "train()\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73074b45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
